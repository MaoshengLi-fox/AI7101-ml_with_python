{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fengSNSWl1X"
   },
   "source": [
    "# Lab 3. Traffic volume prediction.\n",
    "\n",
    "---\n",
    "\n",
    "By the end of this lab you will be able to manipulate huge tabular data:\n",
    "1. Compute different column's statistics (min, max, mean, quantiles etc.);\n",
    "2. Select observations/features by condition/index;\n",
    "3. Create new non-linear combinations of the columns (feature engineering);\n",
    "4. Perform automated data cleaning;\n",
    "\n",
    "and more.\n",
    "\n",
    "---\n",
    "\n",
    "For those who are not familiar with `pandas` we recommend these (alternative) tutorials:\n",
    "\n",
    "1. Single notebook, covers basic pandas functionality (starting with renaming columns ending with using map, apply etc) ~ 30 short examples with links on videos https://nbviewer.jupyter.org/github/justmarkham/pandas-videos/blob/master/pandas.ipynb . Highly recommended for everyone. (about 1-3 hours to go through)\n",
    "\n",
    "2. https://github.com/guipsamora/pandas_exercises/ 11 topics covering all essential functionality with excersises (with solutions).\n",
    "\n",
    "This task will be an easy ride after these tutorials.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Enter your personal data\n",
    "\n",
    "Please enter your first name and last name in the cell below to receive your unique assignment variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your personal data here\n",
    "\n",
    "first_name = \"Qinrong\"\n",
    "last_name = \"Cui\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please fix seeds in all random frameworks.\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from variants import get_variants_for_all_tasks\n",
    "\n",
    "student_variants = get_variants_for_all_tasks(first_name, last_name)\n",
    "\n",
    "def get_task_variant(task_number):\n",
    "    variant = student_variants[f\"task{task_number}\"]\n",
    "    return print(f\"YOUR TASK: {task_number}.{variant}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBv4L3BoWtY8"
   },
   "source": [
    "We are using a public dataset compiling weather information and traffic data continuously monitored in the Twin Cities, Minnesota from 2012 to 2018. The dataset page can be found [here](https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume). We've slightly modified it so please download the dataset provided on Canvas.  \n",
    "\n",
    "You need to download `Metro_Interstate_Traffic_Volume.csv` and place it in the same directory as this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rYmsZ_BQWx6a"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mp5vQW5Xtw5"
   },
   "source": [
    "# 1. Loading data (6 / 100)\n",
    "\n",
    "As always in Data Science you are starting with making nice cup of tea (or coffee). Your next move is to load the data:\n",
    "\n",
    "- Start with loading `Metro_Interstate_Traffic_Volume.csv` file using `pd.read_csv()` function.\n",
    "- You may also want to increase maximal displayed pandas columns: set `pd.options.display.max_columns` to 30\n",
    "- Print top 10 observations in the table. `.head()`\n",
    "- Print last 10 observations in the table. `.tail()`\n",
    "- Print all the data columns names using method `.columns`\n",
    "- Print data size (number of rows and columns). This is the `.shape` of the data.\n",
    "\n",
    "*Almost* every python has a `head` and a `tail` just as DataFrames do.\n",
    "\n",
    "If you are using Google Colab, you can upload the file in the cell below. If you are NOT using Colab, set COLAB_P in the cell below to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "id": "KS_2J7IXX9ZS",
    "outputId": "6a5a75dd-4245-4992-8c2b-20275dceb5cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place your file to the same directory as the notebook, then read your file with pd.read_csv()\n"
     ]
    }
   ],
   "source": [
    "COLAB_P = False\n",
    "if COLAB_P:\n",
    "    print(\"Upload your file, then read it with pd.read_csv()\")\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    fn = list(uploaded.keys())[0]\n",
    "    print(\"File is uploaded to \", fn)\n",
    "else:\n",
    "    print(\"Place your file to the same directory as the notebook, then read your file with pd.read_csv()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IMsqdgKrYugR"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "pd.options.display.max_columns = 30\n",
    "df = pd.read_csv(\"Metro_Interstate_Traffic_Volume.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "H0P12NdQZPxw",
    "outputId": "dc794a4e-c50f-408e-a284-a130a70d2dda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  holiday    temp  rain_1h  snow_1h  clouds_all weather_main  \\\n",
      "0     NaN  288.28      0.0      0.0        40.0       Clouds   \n",
      "1     NaN  289.36      0.0      0.0        75.0       Clouds   \n",
      "2     NaN  289.58      0.0      0.0        90.0       Clouds   \n",
      "3     NaN  290.13      0.0      0.0        90.0       Clouds   \n",
      "4     NaN  291.14      0.0      0.0        75.0       Clouds   \n",
      "5     NaN  291.72      0.0      0.0         1.0        Clear   \n",
      "6     NaN  293.17      0.0      0.0         1.0        Clear   \n",
      "7     NaN  293.86      0.0      0.0         1.0        Clear   \n",
      "8     NaN  294.14      0.0      0.0        20.0       Clouds   \n",
      "9     NaN  293.10      0.0      0.0        20.0       Clouds   \n",
      "\n",
      "  weather_description            date_time  traffic_volume  \n",
      "0    scattered clouds  2012-10-02 09:00:00          5545.0  \n",
      "1       broken clouds  2012-10-02 10:00:00          4516.0  \n",
      "2     overcast clouds  2012-10-02 11:00:00          4767.0  \n",
      "3     overcast clouds  2012-10-02 12:00:00          5026.0  \n",
      "4       broken clouds  2012-10-02 13:00:00          4918.0  \n",
      "5        sky is clear  2012-10-02 14:00:00          5181.0  \n",
      "6        sky is clear  2012-10-02 15:00:00          5584.0  \n",
      "7        sky is clear  2012-10-02 16:00:00          6015.0  \n",
      "8          few clouds  2012-10-02 17:00:00          5791.0  \n",
      "9          few clouds  2012-10-02 18:00:00          4770.0  \n"
     ]
    }
   ],
   "source": [
    "# Observe top 10 observations (int)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "MXEj8wifdVjV",
    "outputId": "2c48f70b-b389-49bb-a9f3-3a506cbaa51f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      holiday    temp  rain_1h  snow_1h  clouds_all  weather_main  \\\n",
      "48194     NaN  283.84     0.00      0.0        75.0          Rain   \n",
      "48195     NaN  283.84     0.00      0.0        75.0       Drizzle   \n",
      "48196     NaN  284.38     0.00      0.0        75.0          Rain   \n",
      "48197     NaN  284.79     0.00      0.0        75.0        Clouds   \n",
      "48198     NaN  284.20     0.25      0.0        75.0          Rain   \n",
      "48199     NaN  283.45     0.00      0.0        75.0        Clouds   \n",
      "48200     NaN  282.76     0.00      0.0        90.0        Clouds   \n",
      "48201     NaN  282.73     0.00      0.0        90.0  Thunderstorm   \n",
      "48202     NaN  282.09     0.00      0.0        90.0        Clouds   \n",
      "48203     NaN  282.12     0.00      0.0        90.0        Clouds   \n",
      "\n",
      "           weather_description            date_time  traffic_volume  \n",
      "48194    proximity shower rain  2018-09-30 15:00:00          4302.0  \n",
      "48195  light intensity drizzle  2018-09-30 15:00:00          4302.0  \n",
      "48196               light rain  2018-09-30 16:00:00          4283.0  \n",
      "48197            broken clouds  2018-09-30 17:00:00          4132.0  \n",
      "48198               light rain  2018-09-30 18:00:00          3947.0  \n",
      "48199            broken clouds  2018-09-30 19:00:00          3543.0  \n",
      "48200          overcast clouds  2018-09-30 20:00:00          2781.0  \n",
      "48201   proximity thunderstorm  2018-09-30 21:00:00          2159.0  \n",
      "48202          overcast clouds  2018-09-30 22:00:00          1450.0  \n",
      "48203          overcast clouds  2018-09-30 23:00:00           954.0  \n"
     ]
    }
   ],
   "source": [
    "# Observe last 10 observations (int)\n",
    "print(df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zui3i6ZOdo2D",
    "outputId": "afbc65c8-ee84-4325-d5c0-9ff307c78f68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['holiday', 'temp', 'rain_1h', 'snow_1h', 'clouds_all', 'weather_main',\n",
      "       'weather_description', 'date_time', 'traffic_volume'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print all the columns/features names (int)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48204, 9)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the dataframe (int)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 1.1\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwPaHlDhuklP",
    "outputId": "b49a1252-f5be-4b1c-be5c-d048af3375d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns ending with a vowel: 2\n"
     ]
    }
   ],
   "source": [
    "# Q1.1 How many columns end with a vowel?\n",
    "# Q1.2 How many columns start with a vowel?\n",
    "# Q1.3 How many columns have `th` in their names?\n",
    "vowels = 'aeiouAEIOU'\n",
    "col_names = df.columns.tolist()\n",
    "col_end_vowel = [col for col in col_names if col[-1] in vowels]\n",
    "print(f\"Number of columns ending with a vowel: {len(col_end_vowel)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 2.1\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ka8e4hmZdqLp",
    "outputId": "fefb08cc-d966-465a-ef55-cb5410f50b43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 9\n"
     ]
    }
   ],
   "source": [
    "# Print data size (int)\n",
    "\n",
    "# Q2.1 How many observations are in the data?\n",
    "# Q2.2 How many features are in the data?\n",
    "\n",
    "print(f\"Number of features: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWqFpMWPdy3E"
   },
   "source": [
    "# 2. Basic data exploration (15 / 100)\n",
    "\n",
    "Lets do some basics:\n",
    "\n",
    "`.count()` number of not NaN's in every column.\n",
    "    \n",
    "Is there any missing values in the data?     \n",
    "Count number of unique values in every column .nunique().    \n",
    "What does this tells you about the features, which are most likely categorical and which are most likely numerical?    \n",
    "Use pandas `.describe()` to display basic statistic about the data.   \n",
    "Use pandas `.value_counts()` to count number of unique values in a specific column.   \n",
    "Use pandas `.min()`, `.max()`, `.mean()`, `.std()` to display specific statistics about the data.    \n",
    "Use pandas `.dtypes` field to display data types in columns. \n",
    "Hint You could use `.sort_index()` or `.sort_values()` to sort the result of `.value_counts()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 3.3\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwlcBdvIwlfB",
    "outputId": "1471393c-97aa-437d-ce98-377d5d62556e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in 'rain_1h': 1\n"
     ]
    }
   ],
   "source": [
    "# Display number of not NaN's in every column (int)\n",
    "\n",
    "# Q3.1 How many NA values are in the `clouds_all` column?\n",
    "# Q3.2 How many NA values are in the `temp` column?\n",
    "# Q3.3 How many NA values are in the `rain_1h` column?\n",
    "# Q3.4 How many NA values are in the `snow_1h` column?\n",
    "# Q3.5 How many explicit NA values are in the `traffic_volume` column?\n",
    "\n",
    "na_counts = df.isna().sum()\n",
    "print(\"Number of missing values in 'rain_1h':\", na_counts['rain_1h'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in 'holiday': 48143\n"
     ]
    }
   ],
   "source": [
    "# How many NA or None values are in the `holiday`?\n",
    "na_counts = df.isna().sum()\n",
    "print(\"Number of missing values in 'holiday':\", na_counts['holiday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in 'holiday' after filling: 0\n"
     ]
    }
   ],
   "source": [
    "# Fill NA's and None values in the `holiday` column with label 'Not_a_holiday'?\n",
    "df['holiday'] = df['holiday'].fillna('Not_a_holiday')\n",
    "na_counts = df.isna().sum()\n",
    "print(\"Number of missing values in 'holiday' after filling:\", na_counts['holiday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NA values. Use dropna.\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "bo6KibQ3x4Jw",
    "outputId": "55e2f0a4-7591-4829-dc61-25b645d56487"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              holiday          temp       rain_1h       snow_1h    clouds_all  \\\n",
      "count           48190  48190.000000  48190.000000  48190.000000  48190.000000   \n",
      "unique             12           NaN           NaN           NaN           NaN   \n",
      "top     Not_a_holiday           NaN           NaN           NaN           NaN   \n",
      "freq            48129           NaN           NaN           NaN           NaN   \n",
      "mean              NaN    281.201366      0.334356      0.000222     49.369267   \n",
      "std               NaN     13.337406     44.795638      0.008169     39.016127   \n",
      "min               NaN      0.000000      0.000000      0.000000      0.000000   \n",
      "25%               NaN    272.160000      0.000000      0.000000      1.000000   \n",
      "50%               NaN    282.440000      0.000000      0.000000     64.000000   \n",
      "75%               NaN    291.800000      0.000000      0.000000     90.000000   \n",
      "max               NaN    310.070000   9831.300000      0.510000    100.000000   \n",
      "\n",
      "       weather_main weather_description            date_time  traffic_volume  \n",
      "count         48190               48190                48190    48190.000000  \n",
      "unique           11                  38                40562             NaN  \n",
      "top          Clouds        sky is clear  2013-04-18 22:00:00             NaN  \n",
      "freq          15159               11658                    6             NaN  \n",
      "mean            NaN                 NaN                  NaN     3259.859079  \n",
      "std             NaN                 NaN                  NaN     1986.972809  \n",
      "min             NaN                 NaN                  NaN        0.000000  \n",
      "25%             NaN                 NaN                  NaN     1192.250000  \n",
      "50%             NaN                 NaN                  NaN     3380.000000  \n",
      "75%             NaN                 NaN                  NaN     4933.000000  \n",
      "max             NaN                 NaN                  NaN     7280.000000  \n"
     ]
    }
   ],
   "source": [
    "# Display basic data statistics using .describe()\n",
    "print(df.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 4.1\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uCxMUIbnx-QD",
    "outputId": "0232e59d-e775-4118-ce07-cb94a96b5b60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in 'clouds_all': 60\n"
     ]
    }
   ],
   "source": [
    "# Count number of unique values in every column (int)\n",
    "\n",
    "# Q4.1 How many unique values are in the `clouds_all` column?\n",
    "# Q4.2 How many unique values are in the `weather_main` column?\n",
    "# Q4.3 How many unique values are in the `weather_description` column?\n",
    "# Q4.4 How many unique values are in the `snow_1h` column?\n",
    "# Q4.5 How many unique values are in the `rain_1h` column?\n",
    "unique_counts = df.nunique()\n",
    "print(\"Number of unique values in 'clouds_all':\", unique_counts['clouds_all'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fPJMcqYQyD9y",
    "outputId": "d1ba998f-0b2b-469d-c6f0-0304fcb5a3f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent value is Clouds with 15159 occurrences.\n"
     ]
    }
   ],
   "source": [
    "# Count frequency of the values in different columns (list of ints in ascending order)\n",
    "# You could select a column using same syntax as for selecting a key from a dictionary: `data[colname]`\n",
    "# numpy's `unique` function can be useful for this task\n",
    "\n",
    "# Q5 What is the most frequent value in the weather_main column?\n",
    "most_freq = df['weather_main'].value_counts().idxmax()\n",
    "count = df['weather_main'].value_counts().max()\n",
    "\n",
    "print(f\"The most frequent value is {most_freq} with {count} occurrences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent value is Clouds with 15159 occurrences.\n"
     ]
    }
   ],
   "source": [
    "# if use numpy's unique\n",
    "values, counts = np.unique(df['weather_main'], return_counts=True)\n",
    "\n",
    "max_index = np.argmax(counts)\n",
    "\n",
    "most_freq = values[max_index]\n",
    "count = counts[max_index]\n",
    "\n",
    "print(f\"The most frequent value is {most_freq} with {count} occurrences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 6.5\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vIOZuv7yErp",
    "outputId": "11c3fdb7-d4bc-4ad2-f059-d4ac01492e2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snow 1h stats (max, min, mean, std): [0.51, 0.0, 0.0, 0.008]\n"
     ]
    }
   ],
   "source": [
    "# Display some column statistics (list of floats, rounded up to 3 digits, e.g. 1.234)\n",
    "\n",
    "# Q6.1 What are the max, min, mean and the std of the `traffic_volume` column?\n",
    "# Q6.2 What are the max, min, mean and the std of the `clouds_all` column?\n",
    "# Q6.3 What are the max, min, mean and the std of the `temp` column?\n",
    "# Q6.4 What are the max, min, mean and the std of the `rain_1h` column?\n",
    "# Q6.5 What are the max, min, mean and the std of the `snow_1h` column?\n",
    "\n",
    "snow_stats = df['snow_1h'].agg(['max', 'min', 'mean', 'std']).round(3)\n",
    "print(\"Snow 1h stats (max, min, mean, std):\", snow_stats.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 7.1\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71Kx1GnoyG7v",
    "outputId": "4748f2d3-ca71-45e7-8438-728672463e43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of object columns: 4\n"
     ]
    }
   ],
   "source": [
    "# Display data types of all columns (int)\n",
    "\n",
    "# Q7.1 How many columns have `object` data type?\n",
    "# Q7.2 How many columns have `int64` data type?\n",
    "# Q7.3 How many columns have `float64` data type?\n",
    "\n",
    "object_count = df.select_dtypes(include='object').shape[1]\n",
    "print(\"Number of object columns:\", object_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rrG2dQQe5yf"
   },
   "source": [
    "# 3. Data selection (15 / 100)\n",
    "\n",
    "In pandas.DataFrame you could select\n",
    "\n",
    "  Row/s by position (integer number [0 .. number of rows - 1]) .iloc or by DataFrame.index .loc:   \n",
    "\n",
    "```\n",
    "  data.loc[0]  \n",
    "  data.loc[5:10]  \n",
    "  data.iloc[0]  \n",
    "  data.iloc[5:10]   \n",
    "```\n",
    "\n",
    "Though, this is probably the worst way to manipulate rows.   \n",
    "  Columns by name\n",
    "\n",
    "```\n",
    "  data[columname]\n",
    "```\n",
    "\n",
    "  Row/s and columns\n",
    "\n",
    "```\n",
    "  data.loc[10, columname]  \n",
    "  data.iloc[10, columname]  \n",
    "```\n",
    "\n",
    "Using boolean mask\n",
    "\n",
    "```\n",
    "  mask = data[columname] > value  \n",
    "  data[mask]  \n",
    "```\n",
    "\n",
    "You could combine multiple conditions using & or | (and, or)   \n",
    "\n",
    "```\n",
    "cond1 = data[columname1] > value1  \n",
    "cond2 = data[columname2] > value2  \n",
    "data[cond1 & cond2]  \n",
    "```\n",
    "\n",
    "Using queries .query():  \n",
    "\n",
    "```\n",
    "value = 5 \n",
    "data.query(\"columname > value\")  \n",
    "```\n",
    "\n",
    "You could combine multiple conditions using and, or  \n",
    "\n",
    "```\n",
    "data.query(\"(columname1 > value1) and (columname2 > value2)\")\n",
    "```\n",
    "\n",
    "and others. See https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html for more examples.\n",
    "\n",
    "Remember to use different quotation marks \" or ' for columnname inside a query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 8.5\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJnCqUx-0YRx",
    "outputId": "0be7619b-9aa3-417c-a1b0-eb4579164d28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datetime at index 2718: 2013-01-06 14:00:00\n"
     ]
    }
   ],
   "source": [
    "# Select rows by position (int) \n",
    "\n",
    "# Q8.1 What is the temperature of the time slot with index 777?\n",
    "# Q8.2 What is the weather description of the time slot with index 999?\n",
    "# Q8.3 How much is cloud coverage with index 1337?\n",
    "# Q8.4 What is the weather main of the time slot with index 314?\n",
    "# Q8.5 When was the time slot with index of 2718 observed?\n",
    "print(\"Datetime at index 2718:\", df.iloc[2718]['date_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 9.3\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nv5jRoeU0aQG",
    "outputId": "635a0c74-7039-40cc-a391-5b31c8823419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloud coverage at index 1045: 20.0\n"
     ]
    }
   ],
   "source": [
    "# Select rows by index (int)\n",
    "\n",
    "# Q9.1 What is the temperature of the time slot on index 1102?\n",
    "# Q9.2 What is the weather description of the time slot on index 5695?\n",
    "# Q9.3 How much is cloud coverage on the index 1045?\n",
    "# Q9.4 What is the weather main of the time slot from index 252?\n",
    "# Q9.5 When was the time slot with index of 38 captured?\n",
    "cloud_1045 = df.loc[1045, 'clouds_all']\n",
    "print(\"Cloud coverage at index 1045:\", cloud_1045)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 10.2\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZneqD6P0e5C",
    "outputId": "e0a1552e-7042-4ead-eb0a-75cf9a213ea5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First light intensity drizzle: 2012-10-10 07:00:00\n"
     ]
    }
   ],
   "source": [
    "# Using mask or .query syntax select rows/columns (int)\n",
    "\n",
    "# Q10.1 How many time slots have less than 270 temperature?\n",
    "# Q10.2 When was the first \"light intensity drizzle\" in weather description captured?\n",
    "# Q10.3 How many time slots have cloud coverage more than 75?\n",
    "# Q10.4 How many time slots are foggy? (weather_main = Fog)\n",
    "# Q10.5 When was the last observed timeslot with weather_description \"heavy snow\"?\n",
    "\n",
    "\n",
    "first_light_intensity_drizzle = df.query(\"weather_description == 'light intensity drizzle'\").iloc[0]['date_time']\n",
    "\n",
    "print(\"First light intensity drizzle:\", first_light_intensity_drizzle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 11.5\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20th Thunderstorm weather_description: proximity thunderstorm\n"
     ]
    }
   ],
   "source": [
    "# Q11.1 What is the traffic volume of November 20th 2016, at 20:00?\n",
    "# Q11.2 What is the amount of rain in the 70th rainy time slot (non-zero rain) of the dataset?\n",
    "# Q11.3 How much cloud coverage percentage were in sky on October 16th 2012 at 19:00?\n",
    "# Q11.4 What is the `traffic_volume` of a thirty fourth sample with `clouds_all` == 90?\n",
    "# Q11.5 What is the \"weather_description\" in the 20th \"weather_main\" with Thunderstorm?\n",
    "\n",
    "thunderstorm_weather_description = df.query(\"weather_main == 'Thunderstorm'\").iloc[19]['weather_description']\n",
    "print(\"20th Thunderstorm weather_description:\", thunderstorm_weather_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 12.1\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic volume for 99-th time slot with clouds_all 75%: 1437.0\n"
     ]
    }
   ],
   "source": [
    "# Q12.1 What is the traffic volume for 99-th time slot with cloud coverage 75 percent?\n",
    "# Q12.2 How much is the temperature the 666-th time slot with weather_description 'proximity thunderstorm'?\n",
    "# Q12.3 What is the temperature of 1337-th time slot with clear sky (clouds_all <= 20)?\n",
    "traffic_99_clouds_75 = df.query(\"clouds_all == 75\").iloc[98]['traffic_volume']\n",
    "print(\"Traffic volume for 99-th time slot with clouds_all 75%:\", traffic_99_clouds_75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBLabnN2fAPc"
   },
   "source": [
    "# 4. Creating new columns (16 / 100)\n",
    "\n",
    "Creating new column of pandas.DataFrame is as easy as:\n",
    "```\n",
    "data['new_awesome_column'] = [] \n",
    "```\n",
    "that's it. But such a column is relatively useless. Typically, you would compute something new based on existing data and save it in a new column. For example one might want to sum a number of existing columns:\n",
    "```\n",
    "data['sum'] = data[col1] + data[col2] + ...\n",
    "```\n",
    "Pandas also provides another powerfull tool: .apply, .map(), .applymap() methods (they are kinda the same, but not quite). https://stackoverflow.com/questions/19798153/difference-between-map-applymap-and-apply-methods-in-pandas . They allow you to apply some function to every value in the column/s (row-wise) or row (column-wise) or cell (element-wise). For example, same computations of sum using .apply():\n",
    "```\n",
    "data['sum'] = data[[col1, col2, col3]].apply(sum, axis=1)\n",
    "```\n",
    "you are not restricted to existent functions, .apply() accepts any function (including lambda functions):\n",
    "```\n",
    "data['sum'] = data[[col1, col2, col3]].apply(lambda x: x[0]+x[1]+x[2], axis=1)\n",
    "```\n",
    "or ordinary python function (if this it should have complex behaviour):\n",
    "```\n",
    "def _sum(x):\n",
    "    total = 0\n",
    "    for elem in x:\n",
    "        total += elem\n",
    "    return total\n",
    "\n",
    "data['sum'] = data[[col1, col2, col3]].apply(_sum, axis=1) \n",
    "```\n",
    "Many pandas methods has axis parameter axis=0 refers to rows, axis=1 refers to columns.\n",
    "\n",
    "Warning. You should never use for loops to sum numerical elements from the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 13.1\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "nxbj9Po2Le18"
   },
   "outputs": [],
   "source": [
    "# Create new columns using the old ones (new column in your DataFrame)\n",
    "\n",
    "# Q13.1 Create a `temp_in_celcius` column from the existing `temp` (kelvin) using any method above\n",
    "# Q13.2 Create a new bool column `hot` which indicates whether the time slot was hot (temp > 300)\n",
    "# Q13.3 Create a new bool column `rainy_and_cloudy` which indicates whether it was rainy (>0.1) AND cloudy (>50)\n",
    "# Q13.4 Create a new bool column `is_holiday` which indicates whether the day of the time slot falls on any holiday\n",
    "# Q13.5 Create a new column `traffic_cat` by splitting a `traffic_volume` into 5 ([1..5]) distinct intervals: 0 < x <=20%,\n",
    "# 20% < x <= 40%, ... 80% < x <= 100% percentiles. You could use `.quantile()` to compute percentiles.\n",
    "\n",
    "df['temp_in_celsius'] = df['temp'] - 273.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns after feature extraction: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of columns after feature extraction:\", df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 14.5\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xm4Ve-s70h73",
    "outputId": "d68d003c-151e-4803-9823-5c8d1686fff3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum traffic volume in June 2017 with clear sky: 6673.0\n"
     ]
    }
   ],
   "source": [
    "# Using mask or .query syntax select rows/columns (int).\n",
    "# For working with dates, define helper functions that operate on the date_time string.\n",
    "\n",
    "# Q14.1 How many time slots with \"weather_main\" equal to \"Clouds\"  were captured in autumn 2016? Including both start and end day.\n",
    "# Q14.2 How many rainy time slots that were captured in the fall, with traffic volume more than 2000?\n",
    "# Q14.3 How many time slots that are warmer than 270, have weather main \"Clouds\"?\n",
    "# Q14.4 What is the minimum traffic volume of time slots captured on March 8th (all years), that was warmer than 290?\n",
    "# Q14.5 How much is the maximum traffic volume for the time slots were captured in June 2017 and has clear sky (weather_main)?\n",
    "\n",
    "max_traffic_june_2017_clear_sky = df.query(\"weather_main == 'Clear' and date_time >= '2017-06-01' and date_time < '2017-07-01'\")['traffic_volume'].max()\n",
    "print(\"Maximum traffic volume in June 2017 with clear sky:\", max_traffic_june_2017_clear_sky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 15.3\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TTVitIYf0ib3",
    "outputId": "1300c6b6-032e-4497-ef1a-7b3c8497bf09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traffic volume at highest snow in one hour: 5167.0\n"
     ]
    }
   ],
   "source": [
    "# Using mask or .query syntax select rows/columns and compute simple statistics (float)\n",
    "\n",
    "# Q15.1 What was the average temperature of time slots with main weather \"Haze\"?\n",
    "# Q15.2 What was the traffic volume of the coldest time slot of the year 2016?\n",
    "# Q15.3 What was the traffic volume of the highest amount of snow in one hour?\n",
    "# Q15.4 What is the median of temperatures captured in April 2017?\n",
    "# Q15.5 What is the maximum temperature of time slots with broken clouds?\n",
    "\n",
    "traffic_highest_snow = df.loc[df['snow_1h'].idxmax(), 'traffic_volume']\n",
    "print(\"Traffic volume at highest snow in one hour:\", traffic_highest_snow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 16.4\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fzig1HmlL4rq",
    "outputId": "9ebe4adf-0811-4acc-85e9-7907180880a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average traffic volume in highest quantile: 5705.782172470978\n"
     ]
    }
   ],
   "source": [
    "# Using mask or .query syntax select rows/columns (float)\n",
    "\n",
    "# Q16.1 What is the average temperature of the time slots with weather_main=thunderstorm?\n",
    "# Q16.2 What is the average traffic volume on holidays?\n",
    "# Q16.3 What is the average traffic volume on non-holidays?\n",
    "# Q16.4 What is the average traffic volume in the highest quantile?\n",
    "# Q16.5 What is the average traffic volume in the lowest quantile?\n",
    "\n",
    "q75 = df['traffic_volume'].quantile(0.75)\n",
    "\n",
    "avg_highest_quantile = df.loc[df['traffic_volume'] >= q75, 'traffic_volume'].mean()\n",
    "\n",
    "print(\"Average traffic volume in highest quantile:\", avg_highest_quantile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezvSnhPRfFXM"
   },
   "source": [
    "# 5. Basic date processing (8 / 100)\n",
    "\n",
    "You figure out that column date is to harsh for you, so you decided to convert it to a more plausible format:\n",
    "\n",
    "- Use pandas method to_datetime() to convert the date to a good format.\n",
    "- Extract year, month, day and weekday from your new date column. Save them to separate columns.\n",
    "- How many columns has your data now?\n",
    "- Drop column date, remember to set inplace parameter to True.\n",
    "\n",
    "Hint: for datetime formatted date you could extract the year as follow:\n",
    "```\n",
    "data.date.dt.year\n",
    "```\n",
    "Very often date could be a ridiculously rich feature, sometimes it is holidays that matters, sometimes weekends, sometimes some special days like black friday.\n",
    "\n",
    "Learn how to work with date in Python!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 17.3\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns after feature extraction: 15\n"
     ]
    }
   ],
   "source": [
    "# Create new columns based on `Captured` column\n",
    "\n",
    "# Q17.1 Extract and store `year`\n",
    "# Q17.2 Extract and store `month`\n",
    "# Q17.3 Extract and store `day`\n",
    "# Q17.4 Extract and store `weekday` (Monday - 0, Sunday - 6)\n",
    "# Q17.5 Extract and store `hour`\n",
    "# Q17.6 Extract and store `is_hoilday`\n",
    "\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "df['year'] = df['date_time'].dt.year\n",
    "df['month'] = df['date_time'].dt.month\n",
    "df['day'] = df['date_time'].dt.day\n",
    "df['weekday'] = df['date_time'].dt.weekday  \n",
    "df['hour'] = df['date_time'].dt.hour\n",
    "print(\"Number of columns after feature extraction:\", df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 18.4\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UU_wYcyETK9T",
    "outputId": "4869f3a1-0740-4830-e014-70e1da8ba8e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average traffic volume between 15â€“19 hours: 4749.295714285714\n"
     ]
    }
   ],
   "source": [
    "# Find some date related information from the data (int)\n",
    "\n",
    "# Q18.1 What is the weekday with the highest traffic volume?\n",
    "# Q18.2 What is the weekday with the lowest traffic volume?\n",
    "# Q18.3 What is the average traffic volume during months of September?\n",
    "# Q18.4 What is the average traffic volume in the time period between 15-19 hours\n",
    "# Q18.5 What is the average traffic volume on World Bicycle Day (June 3)?\n",
    "\n",
    "\n",
    "avg_traffic_15_19 = df.loc[df['hour'].between(15, 19), 'traffic_volume'].mean()\n",
    "print(\"Average traffic volume between 15â€“19 hours:\", avg_traffic_15_19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LrPCPd0fRYz"
   },
   "source": [
    "# 6. Groupby (5 / 100)\n",
    "\n",
    "from the documentation https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html\n",
    "\n",
    "By â€œgroup byâ€ we are referring to a process involving one or more of the following steps:\n",
    "\n",
    "- Splitting the data into groups based on some criteria.\n",
    "- Applying a function to each group independently.\n",
    "- Combining the results into a data structure.\n",
    "\n",
    "`.groupby()` is one of the most powerfull tool for feature engineering. Very often it is used to group object with the same categorical characteristics and compute some statistics (e.g. mean, max, etc.) of a their numerical characteric.\n",
    "\n",
    "Instead of computing average traffic volume with for each month you could compute average traffic volumes for every month in a single command:\n",
    "```\n",
    "data.groupby('month')['traffic_volume'].mean()\n",
    "```\n",
    "You could also make multi-column groups:\n",
    "```\n",
    "data.groupby(['weekday','month'])['traffic_volume'].min()\n",
    "```\n",
    "next, you could compute multiple aggregation functions:\n",
    "```\n",
    "data.groupby(['weekday','month'])['traffic_volume'].agg([min, max])\n",
    "```\n",
    "instead of using built-in functions you could compute custom functions using apply:\n",
    "```\n",
    "import numpy as np\n",
    "data.groupby(['weekday','month'])['traffic_volume'].apply(lambda x: np.quantile(x, .5))\n",
    "```\n",
    "and the coolest thing now is that you can map the results of groupby back on your DataFrame!\n",
    "```\n",
    "gp = data.groupby(['month'])['traffic_volume'].median()\n",
    "data['gp_feature'] = data['month'].map(gp)\n",
    "```\n",
    "Now, if some timeslot has month == 2, its gp_feature will be equal to the median traffic volume amongst all observations in February\n",
    "\n",
    "Read more examples in the documentation https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 19.1\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "nRmmkfIVXp5K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q19.1 Median traffic volume in 2013: 3344.0\n"
     ]
    }
   ],
   "source": [
    "# Create some groupby features\n",
    "\n",
    "# Q19.1 `traffic_by_year` groupby `year` and compute median traffic volume in 2013.\n",
    "# Q19.2 `traffic_by_weekday` groupby `weekday` and compute median traffic volume on Monday.\n",
    "\n",
    "traffic_by_year = df.groupby('year')['traffic_volume'].median()\n",
    "\n",
    "median_2013 = traffic_by_year.loc[2013]\n",
    "print(\"Q19.1 Median traffic volume in 2013:\", median_2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Y3IX6U1fOVI"
   },
   "source": [
    "# 7. Building a regression model (35 / 100)\n",
    "\n",
    "- You do not need to normalize data for tree models, and for linear/knn models this step is essential.\n",
    "- Remember, that not all of the features in the table are numeric, some of them might be viewed as categorical.\n",
    "- You may create or drop any features you want - try to only keep features which you think will be relevant to the prediction of traffic volume.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "AwdbUu2wYKpB"
   },
   "outputs": [],
   "source": [
    "# Q20 Separate your data into inputs and targets, keeping only relevant inputs. Drop any features computed from the output eg. `traffic_cat`\n",
    "Y = df[\"traffic_volume\"].values\n",
    "\n",
    "good_columns = ['temp',\n",
    "    'rain_1h',\n",
    "    'snow_1h',\n",
    "    'clouds_all',\n",
    "    'holiday',\n",
    "    'weather_main',\n",
    "    'weather_description',\n",
    "    'year',\n",
    "    'month',\n",
    "    'day',\n",
    "    'weekday',\n",
    "    'hour'] #todo\n",
    "Xdf = df[good_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to split our data into train and test sets. Generally a random split is used, but one needs to be very careful with time series data - we need to make sure train and test data don't contain mixed adjacent time slots. In general with time series, it is recommended not to predict values from the past using input information from the future (although the applicability of this rule in our case is debatable), so we'll use sklearn's [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) class here. TimeSeriesSplit splits data into a number of folds, then only provides data from past folds to train a model tested on the currently considered fold. So if we split our data into five parts, we'll get four folds:\n",
    "\n",
    "1. Train on [0], test on [1]\n",
    "2. Train on [0,1], test on [2]\n",
    "3. Train on [0, 1, 2], test on [3]\n",
    "4. Train on [0, 1, 2, 3], test on [4]\n",
    "\n",
    "For the following tasks, you are required to use train and test indices from the last fold provided by TimeSeriesSplit with `n_splits` = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdLqDhuCYO1Z",
    "outputId": "99703cb2-7dc5-4987-cd29-7f1a149a2b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 40159\n",
      "Test size: 8031\n"
     ]
    }
   ],
   "source": [
    "# Q21 Split your data into train and test parts.\n",
    "# How many records (rows) do you have in train and test tables? (list of int)?\n",
    "# Use sklearn.model_selection.TimeSeriesSplit with n_splits=5\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "Xdf_encoded = pd.get_dummies(Xdf, drop_first=True)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_idx, test_idx in tscv.split(Xdf_encoded):\n",
    "    pass\n",
    "\n",
    "X_train, X_test = Xdf_encoded.iloc[train_idx], Xdf_encoded.iloc[test_idx]\n",
    "y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0])\n",
    "print(\"Test size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 22.3\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xqubzK5rYQA9",
    "outputId": "d9853eb1-2a3c-4eb2-e01a-aaceeb6d5b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN MSE: 1267970.4679666292\n",
      "KNN R^2: 0.6731068671401076\n"
     ]
    }
   ],
   "source": [
    "# Create a predictive regression model of a traffic volume.\n",
    "\n",
    "# Q22.1 Use linear regression with l2 regularization (Ridge regression)\n",
    "# Q22.2 Use decision tree regression\n",
    "# Q22.3 Use k nearest neighbours regression\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "knn_pipe = make_pipeline(StandardScaler(), KNeighborsRegressor(n_neighbors=5))\n",
    "knn_pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = knn_pipe.predict(X_test)\n",
    "print(\"KNN MSE:\", mean_squared_error(y_test, y_pred_knn))\n",
    "print(\"KNN R^2:\", r2_score(y_test, y_pred_knn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 23.1\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "7uWxRty0YSBd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Ridge Alpha: 100.0\n"
     ]
    }
   ],
   "source": [
    "# Use grid search to select optimal hyperparamters of your models. \n",
    "\n",
    "# Q23.1 Alpha for a ridge regression\n",
    "# Q23.2 Depth for the tree\n",
    "# Q23.3 Number of neighbours for the knn\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "ridge_grid = GridSearchCV(\n",
    "    make_pipeline(StandardScaler(), Ridge()),\n",
    "    param_grid={'ridge__alpha': [0.1, 1.0, 10.0, 100.0]},\n",
    "    cv=tscv,\n",
    "    scoring='neg_mean_squared_error',\n",
    ")\n",
    "ridge_grid.fit(Xdf_encoded, Y)\n",
    "best_alpha = ridge_grid.best_params_['ridge__alpha']\n",
    "print(\"Best Ridge Alpha:\", best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 24.1\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IVihHjlpYTrB",
    "outputId": "755e3cb5-acf6-4022-88a8-188cba883bef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Train MSE: 3285522.481602991\n",
      "Ridge Test  MSE: 3233160.087316947\n"
     ]
    }
   ],
   "source": [
    "# Compute train and test mean squared error for your best models (list of float).\n",
    "\n",
    "# Q24.1 Train, test MSE using linear regression with l2 regularization\n",
    "# Q24.2 Train, test MSE using decision tree regression\n",
    "# Q24.3 Train, test MSE using k nearest neighbours regression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "ridge_pipe = make_pipeline(StandardScaler(), Ridge(alpha=best_alpha))\n",
    "ridge_pipe.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = ridge_pipe.predict(X_train)\n",
    "y_test_pred  = ridge_pipe.predict(X_test)\n",
    "\n",
    "print(\"Ridge Train MSE:\", mean_squared_error(y_train, y_train_pred))\n",
    "print(\"Ridge Test  MSE:\", mean_squared_error(y_test,  y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR TASK: 25.3\n"
     ]
    }
   ],
   "source": [
    "get_task_variant(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqwSIwVmYVZ6",
    "outputId": "c8bc2903-7def-4076-b7ba-b13cd2235b60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.8216024358999575), np.float64(0.6731068671401076)]\n"
     ]
    }
   ],
   "source": [
    "# Compute train and test R^2 for your best models (list of float).\n",
    "\n",
    "# Q25.1 Train, test R^2 using linear regression with l2 regularization\n",
    "# Q25.2 Train, test R^2 using decision tree regression\n",
    "# Q25.3 Train, test R^2 using k nearest neighbours regression\n",
    "\n",
    "knn_pipe = make_pipeline(StandardScaler(), KNeighborsRegressor(n_neighbors=5))\n",
    "knn_pipe.fit(X_train, y_train)\n",
    "\n",
    "y_tr_pred = knn_pipe.predict(X_train)\n",
    "y_te_pred = knn_pipe.predict(X_test)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_tr_pred)\n",
    "test_r2  = r2_score(y_test,  y_te_pred)\n",
    "print([train_r2, test_r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "qO3zWCyHYXQo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 features (by abs weight): ['hour', 'weekday', 'clouds_all', 'temp', 'weather_description_scattered clouds']\n"
     ]
    }
   ],
   "source": [
    "# Q26 Which features have largest (by absolute value) weight in your linear model (top 5 features)? (list of str).\n",
    "\n",
    "ridge_final = make_pipeline(StandardScaler(), Ridge(alpha=best_alpha))\n",
    "ridge_final.fit(X_train, y_train)\n",
    "\n",
    "ridge_model = ridge_final.named_steps['ridge']\n",
    "\n",
    "coefs = ridge_model.coef_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "top5_idx = np.argsort(np.abs(coefs))[-5:][::-1]\n",
    "top5_features = feature_names[top5_idx].tolist()\n",
    "\n",
    "print(\"Top 5 features (by abs weight):\", top5_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9G_LlOHofTJp"
   },
   "source": [
    "# Make sure your .ipynb is linearly executable     \n",
    "# Kernel -> Restart & Run All -> No ERROR cells"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CS4120",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
